import sys
import logging
import crawldb
from crawldb.heritrix import CrawlLogLine
import psycopg2
from psycopg2.extras import execute_values
import luigi
import luigi.contrib.hdfs
import luigi.contrib.hadoop
from luigi.contrib.hdfs.format import Plain, PlainDir

import dateutil, six, idna, urlcanon, urllib3, certifi, chardet, requests, requests_file, tldextract

logger = logging.getLogger(__name__)


class InputFile(luigi.ExternalTask):
    """
    This ExternalTask defines the Target at the top of the task chain. i.e. resources that are overall inputs rather
    than generated by the tasks themselves.
    """
    path = luigi.Parameter()
    from_hdfs = luigi.BoolParameter(default=False)

    def output(self):
        """
        Returns the target output for this task.
        In this case, it expects a file to be present in HDFS.
        :return: the target output for this task.
        :rtype: object (:py:class:`luigi.target.Target`)
        """
        if self.from_hdfs:
            return luigi.contrib.hdfs.HdfsTarget(path=self.path)
        else:
            return luigi.LocalTarget(path=self.path)


class SendLogFileToCrawlDB(luigi.contrib.hadoop.JobTask):
    """
    Map-Reduce job that pushes crawl log data to a PostgreSQL-compliant Crawl DB

    """

    task_namespace = 'analyse'
    log_paths = luigi.ListParameter()
    from_hdfs = luigi.BoolParameter(default=False)
    cdb_db = luigi.Parameter(default='defaultdb')
    cdb_user = luigi.Parameter(default='root')
    cdb_password = luigi.Parameter(default=None)
    cdb_port = luigi.IntParameter(default=26257)
    cdb_host = luigi.Parameter(default='localhost')
    batch_size = luigi.IntParameter(default=1000)

    # This can be set to 1 if there is intended to be one output file. The usual Luigi default is 25.
    # Using one output file ensures the whole output is sorted but is not suitable for very large crawls.
    n_reduce_tasks = luigi.IntParameter(default=1)

    # DB connection:
    conn = None
    cur = None

    def requires(self):
        reqs = []
        for log_path in self.log_paths:
            logger.info("LOG FILE TO PROCESS: %s" % log_path)
            reqs.append(InputFile(log_path, self.from_hdfs))
        return reqs

    def output(self):
        out_name = "task-state/crawl-logs-to-db-%s.log" % (self.task_id)
        if self.from_hdfs:
            return luigi.contrib.hdfs.HdfsTarget(path=out_name, format=PlainDir)
        else:
            return luigi.LocalTarget(path=out_name)

    def extra_modules(self):
        return [crawldb,psycopg2,dateutil,six,idna,urlcanon,urllib3,certifi,chardet,requests,requests_file,tldextract]

    def init_mapper(self):
        # Set up DB connection...
        self.conn = psycopg2.connect(
            database=self.cdb_db,
            user=self.cdb_user,
            password=self.cdb_password,
            sslmode='disable',
            port=self.cdb_port,
            host=self.cdb_host,
        )
        # Make each statement commit immediately.
        self.conn.set_session(autocommit=True)

        # Open a cursor to perform database operations.
        self.cur = self.conn.cursor()

    def run_mapper(self, stdin=sys.stdin, stdout=sys.stdout):
        """
        Run the mapper on the hadoop node.

        UKWA: In this case, we modify the mapper behaviour to make efficient, batched SQL inserts possible.
        """
        self.init_hadoop()
        self.init_mapper()
        execute_values(
            self.cur,
            CrawlLogLine.upsert_sql,
            self._map_input((line[:-1] for line in stdin)),
            page_size=self.batch_size
        )
        outputs = []
        if self.reducer == NotImplemented:
            self.writer(outputs, stdout)
        else:
            self.internal_writer(outputs, stdout)

    def mapper(self, line):
        # Parse:
        c = CrawlLogLine(line)
        yield c.upsert_values()

    def reducer(self, key, values):
        """
        A pass-through reducer.

        :param key:
        :param values:
        :return:
        """
        # Just pass through:
        for value in values:
            yield key, value


if __name__ == '__main__':
    luigi.run(['analyse.SendLogFileToCrawlDB', '--log-paths', '[ "/Users/andy/Documents/workspace/wren/compose-dev-crawler/output/logs/frequent/crawl.log.20160331160522" ]',
               '--local-scheduler'])

    #luigi.run(['analyse.SummariseLogFiles', '--cdb-host', 'bigcdx',
    #           '--log-paths', '[ "/heritrix/output/logs/dc0-20170515/crawl.log.cp00001-20170610062435" ]',
    #           '--on-hdfs', '--local-scheduler'])

    # luigi.run(['analyse.AnalyseLogFile', '--job', 'weekly', '--launch-id', '20170220090024',
    #           '--log-paths', '[ "/Users/andy/Documents/workspace/pulse/python-shepherd/tasks/process/extract/test-data/crawl.log.cp00001-20170211224931", "/Users/andy/Documents/workspace/pulse/python-shepherd/tasks/process/extract/test-data/crawl.log.cp00001-20130605082749" ]',
    #           '--targets-path', '/Users/andy/Documents/workspace/pulse/python-shepherd/tasks/process/extract/test-data/crawl-feed.2017-01-02T2100.frequent',
    #           '--local-scheduler'])

    # luigi.run(['analyse.AnalyseLogFiles', '--date-interval', '2017-02-10-2017-02-12', '--local-scheduler'])
    # luigi.run(['analyse.AnalyseLogFile', '--job', 'weekly', '--launch-id', '20170220090024', '--local-scheduler'])
