import os
import sys
import logging
import crawldb
from crawldb.heritrix import CrawlLogLine
import psycopg2
from psycopg2.extras import execute_values
import luigi
import luigi.contrib.hdfs
import luigi.contrib.hadoop
from luigi.contrib.hdfs.format import Plain, PlainDir

import dateutil, six, idna, urlcanon, urllib3, certifi, chardet, requests, requests_file, tldextract

logger = logging.getLogger(__name__)


class InputFile(luigi.ExternalTask):
    """
    This ExternalTask defines the Target at the top of the task chain. i.e. resources that are overall inputs rather
    than generated by the tasks themselves.
    """
    path = luigi.Parameter()
    from_hdfs = luigi.BoolParameter(default=False)

    def output(self):
        """
        Returns the target output for this task.
        In this case, it expects a file to be present in HDFS.
        :return: the target output for this task.
        :rtype: object (:py:class:`luigi.target.Target`)
        """
        if self.from_hdfs:
            return luigi.contrib.hdfs.HdfsTarget(path=self.path)
        else:
            return luigi.LocalTarget(path=self.path)


class SendLogFileToCrawlDB(luigi.contrib.hadoop.JobTask):
    """
    Map-Reduce job that pushes crawl log data to a PostgreSQL-compliant Crawl DB

    """

    task_namespace = 'analyse'
    crawl_job_name = luigi.Parameter(default='frequent')
    log_paths = luigi.ListParameter()
    from_hdfs = luigi.BoolParameter(default=False)
    cdb_db = luigi.Parameter(default='crawl_db')
    cdb_user = luigi.Parameter(default='root')
    cdb_password = luigi.Parameter(default=None)
    cdb_port = luigi.IntParameter(default=26257)
    cdb_host = luigi.Parameter(default='localhost')
    batch_size = luigi.IntParameter(default=1000)

    # This can be set to 1 if there is intended to be one output file. The usual Luigi default is 25.
    # Using one output file ensures the whole output is sorted but is not suitable for very large crawls.
    n_reduce_tasks = luigi.IntParameter(default=1)

    # Line counter used to ID events:
    line_counter = 0

    # Storing the pervious line (to catch key collisions)
    last_c = None

    # DB connection:
    conn = None
    cur = None

    def requires(self):
        reqs = []
        for log_path in self.log_paths:
            logger.info("LOG FILE TO PROCESS: %s" % log_path)
            reqs.append(InputFile(log_path, self.from_hdfs))
        return reqs

    def output(self):
        out_name = "task-state/crawl-logs-to-db-%s.log" % (self.task_id)
        if self.from_hdfs:
            return luigi.contrib.hdfs.HdfsTarget(path=out_name, format=PlainDir)
        else:
            return luigi.LocalTarget(path=out_name)

    def jobconfs(self):
        # Configure to avoid speculative execution...
        jcs = super(SendLogFileToCrawlDB, self).jobconfs()
        # Newer syntax:
        jcs.append('mapreduce.map.speculative=false')
        jcs.append('mapreduce.reduce.speculative=false')
        # Older syntax (e.g. 0.20.x)
        jcs.append('mapred.map.tasks.speculative.execution=false')
        jcs.append('mapred.reduce.tasks.speculative.execution=false')

        return jcs

    def extra_modules(self):
        return [crawldb,psycopg2,dateutil,six,idna,urlcanon,urllib3,certifi,chardet,requests,requests_file,tldextract]

    def extra_files(self):
        return ['_tld_extract_cache']

    def init_mapper(self):
        # Set up DB connection...
        logger.info("Getting DB connection...")
        self.conn = psycopg2.connect(
            database=self.cdb_db,
            user=self.cdb_user,
            password=self.cdb_password,
            sslmode='disable',
            port=self.cdb_port,
            host=self.cdb_host,
        )
        # Make each statement commit immediately.
        logger.info("Configuring DB connection...")
        self.conn.set_session(autocommit=True)

        # Open a cursor to perform database operations.
        logger.info("Getting DB cursor...")
        self.cur = self.conn.cursor()

    def run_mapper(self, stdin=sys.stdin, stdout=sys.stdout):
        """
        Run the mapper on the hadoop node.

        UKWA: In this case, we modify the mapper behaviour to make efficient, batched SQL inserts possible.
        """
        logger.warning("Initialising...")
        self.init_hadoop()
        self.init_mapper()
        logger.warning("Initialised. Now starting execute_values batch submission...")
        execute_values(
            self.cur,
            CrawlLogLine.upsert_sql,
            self._map_input_reporter(stdin, stdout),
            page_size=self.batch_size
        )

    def _map_input_reporter(self, stdin, stdout):
        counter = 0
        for result in self._map_input((line[:-1] for line in stdin)):
            if counter%1000 == 0:
                outputs = [("STATUS", "Emitted %i lines" % counter)]
                if self.reducer == NotImplemented:
                    self.writer(outputs, stdout)
                else:
                    self.internal_writer(outputs, stdout)
            counter += 1
            yield result

    def mapper(self, line):
        # Parse:
        c = CrawlLogLine(line, job_name=self.crawl_job_name, log_filename=os.getenv('map_input_file', None))

        # Yield this line if there seems there is no key collision with the previous line:
        if self.last_c and c.ssurt == self.last_c.ssurt and c.timestamp == self.last_c.timestamp:
            logger.warning("Skipping line %i because the last line appears to collide with this one." % self.line_counter)
            logger.warning("Prev line %s" % self.last_c.line)
            logger.warning("Curr line %s" % c.line)
        else:
            yield c.upsert_values()

        # Report on progress:
        if self.line_counter % 1000 == 0:
            logger.warning("Processed %i lines..." % self.line_counter)
        self.line_counter += 1

        # Remember this line as the last line:
        self.last_c = c

    def reducer(self, key, values):
        """
        A pass-through reducer.

        :param key:
        :param values:
        :return:
        """
        # Just pass through:
        for value in values:
            yield key, value


if __name__ == '__main__':
    luigi.run(['analyse.SendLogFileToCrawlDB', '--log-paths', '[ "/Users/andy/Documents/workspace/wren/compose-dev-crawler/output/logs/frequent/crawl.log.20160331160522" ]',
               '--local-scheduler'])

    #luigi.run(['analyse.SummariseLogFiles', '--cdb-host', 'bigcdx',
    #           '--log-paths', '[ "/heritrix/output/logs/dc0-20170515/crawl.log.cp00001-20170610062435" ]',
    #           '--on-hdfs', '--local-scheduler'])

    # luigi.run(['analyse.AnalyseLogFile', '--job', 'weekly', '--launch-id', '20170220090024',
    #           '--log-paths', '[ "/Users/andy/Documents/workspace/pulse/python-shepherd/tasks/process/extract/test-data/crawl.log.cp00001-20170211224931", "/Users/andy/Documents/workspace/pulse/python-shepherd/tasks/process/extract/test-data/crawl.log.cp00001-20130605082749" ]',
    #           '--targets-path', '/Users/andy/Documents/workspace/pulse/python-shepherd/tasks/process/extract/test-data/crawl-feed.2017-01-02T2100.frequent',
    #           '--local-scheduler'])

    # luigi.run(['analyse.AnalyseLogFiles', '--date-interval', '2017-02-10-2017-02-12', '--local-scheduler'])
    # luigi.run(['analyse.AnalyseLogFile', '--job', 'weekly', '--launch-id', '20170220090024', '--local-scheduler'])
